{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Colenda Items by Place\n",
    "\n",
    "[Colenda Digital Repository at Penn Libraries](https://colenda.library.upenn.edu/) is a digital repository for digitized and born-digital material. It provides direct access and long-term stewardship for these important resources. Much of Colenda’s content consists of materials owned and digitized by the Penn Libraries, including significant collections that have been donated.\n",
    "\n",
    "In this notebook we'll explore the spatial dimensions of data harvested from Colenda. What places are associated with these items? To do that we'll extract the spatial data, see what's there, and create a few maps.\n",
    "\n",
    "[See here](kaplan_explore_records.ipynb) for an introduction to exploring Colenda data, and [here to explore items in a collection over time](kaplan_explore_time.ipynb).\n",
    "\n",
    "\n",
    "* [Import What We Need](#Import-What-We-Need)\n",
    "* [Load the Data](#Load-the-Data)\n",
    "* [Concatenate and Split `metadata.geographic_subject` Fields](#Concatenate-and-Split-`metadata.geographic_subject`-Fields)\n",
    "* [Geocode `geographic_subject` Data with Nominatim](#Geocode-geographic_subject-Data-with-Nominatim)\n",
    "* [Map Geographic Subjects on a World Map](#Map-Geographic-Subjects-on-a-World-Map)\n",
    "* [Map Geographic Subjects on a US Map](#Map-Geographic-Subjects-on-a-US-Map)\n",
    "* [Enrich Geograhpic Subject Data](#Enrich-Geographic-Subject-Data)\n",
    "* [Filter Items by US State](#Filter-Items-by-US-State)\n",
    "* [Count Items by US State](#Count-Items-by-US-State)\n",
    "* [Map Geographic Subjects on a US State Map](#Map-Geographic-Subjects-on-a-US-State-Map)\n",
    "* [Need Help?](#Need-Help?)\n",
    "* [Credits](#Credits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p><b>Yellow blocks like this provide additional information about Python and Jupyter notebooks.</b></p>\n",
    "    \n",
    "<p>If you haven't used one of these notebooks before, they're basically web pages in which you can write, edit, and run live code. They're meant to encourage experimentation, so don't feel nervous. Just try running a few cells and see what happens!</p>\n",
    "\n",
    "<p>\n",
    "    Some tips:\n",
    "    <ul>\n",
    "        <li>Code cells have boxes around them.</li>\n",
    "        <li>To run a code cell click on the cell and then hit <b>Shift+Enter</b>. The <b>Shift+Enter</b> combo will also move you to the next cell, so it's a quick way to work through the notebook.</li>\n",
    "        <li>While a cell is running a <b>*</b> appears in the square brackets next to the cell. Once the cell has finished running the asterix will be replaced with a number.</li>\n",
    "        <li>In most cases you'll want to start from the top of notebook and work your way down running each cell in turn. Later cells might depend on the results of earlier ones.</li>\n",
    "        <li>To edit a code cell, just click on it and type stuff. Remember to run the cell once you've finished editing.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "\n",
    "<p><b>Is this thing on?</b> If you can't edit or run any of the code cells, you might be viewing a static (read only) version of this notebook. Click here to <a href=\"https://mybinder.org/v2/gh/GLAM-Workbench/national-museum-australia/master?urlpath=lab%2Ftree%2Fexplore_collection_object_over_time.ipynb\">load a <b>live</b> version</a> running on Binder.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import What We Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <p>In order to use this notebook, you first need to `import` modules and packages from Python.</p>\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p>These modules and packages are units of code with specific tools or skills that we use in the script. If you're running this notebook on your computer, you may need to first `import` these modules within your Python interpreter. Find assistance for that <a href=\"https://packaging.python.org/tutorials/installing-packages/\">here</a>.</p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "\n",
    "# Pandas is a Python package that provides numerous tools for data analysis\n",
    "import pandas as pd\n",
    "\n",
    "# IpyLeaflet is a Python library that enables interactive geospatial data visualization in Jupyter Notebook\n",
    "from ipyleaflet import Map, Marker, Popup, MarkerCluster, basemap_to_tiles, CircleMarker\n",
    "\n",
    "# IpyWidgets is a Python library that enables interactive HTML widgets for Jupyter notebooks.\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Reverse Geocode is a Python module that takes latitude / longitude coordinate and returns the country and city \n",
    "import reverse_geocode\n",
    "\n",
    "# Altair is a Python library for declarative statistical visualization\n",
    "import altair as alt\n",
    "\n",
    "# IPython is a Python interpreter to display content\n",
    "from IPython.display import display, HTML, FileLink\n",
    "\n",
    "# Vega_Datasets is a Python package for example datasets - we use it for maps\n",
    "from vega_datasets import data as vega_data\n",
    "\n",
    "#Altair_Saver saves the output of your chart as an external output\n",
    "from altair_saver import save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pre-harvested dataset from Colenda includes many gifts of [Arnold and Deanne Kaplan](https://kaplan.exhibits.library.upenn.edu/thekaplans), which is concentrated in two collections. In this notebook we will only work with records from the Arnold and Deanne Kaplan Collection of **Early American Judaica**. We can access those items by using the `metadata.collection[1]` column and filtering on the Early American Judaica collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a dataframe\n",
    "df = pd.read_csv(\"data/kaplan-test-data.csv\", encoding= 'unicode_escape')\n",
    "\n",
    "# Print the number of rows in the dataframe\n",
    "print('There are {:,} items in this dataset from Colenda.'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p>You may see a warning appear above stating that columns having \"mixed types\". This means that the CSV columns contain a mix of strings and integers. When converting the CSV into a dataframe, Python wasn't sure how to declare the column type. It's OK to ignore this warning for now - we may need to state directly this information later.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for items from the Early American Judaica Collection\n",
    "df = df.loc[df['metadata.collection[1]'] == \"Arnold and Deanne Kaplan Collection of Early American Judaica (University of Pennsylvania)\"]\n",
    "\n",
    "# Return the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate and Split `metadata.geographic_subject` Fields\n",
    "\n",
    "Now that we have this dataset in a dataframe, we can manipulate it. This dataset contains **descriptive metadata** about the items in the collection, which provides information about the intellectual content of a digital object. Descriptive metadata documents and tracks the intellectual content of an item, as well as support the search and discovery of these items within Colenda. The most important field of descriptive metadata is a unique identifier that uniquely identifies the object. Other descriptive metadata fields may include title, author, date of publication, subject, publisher and description. \n",
    "\n",
    "Locations are linked to item records through the `metadata.geographic_subject` columns. One item record could reference multiple locations - for example, an item from `United States -- Pennsylvania -- Philadelphia` would also be listed as `United States -- Pennsylvania`. For comparative and quantitative data analysis, we need to split those items into multiple rows instead of columns.\n",
    "\n",
    "We'll write two **functions** help us do that: \n",
    "* `tidy_split` splits the values of each cell on a \"|\" so that there is one split value per row\n",
    "* `tidy_concat` concatenates (combines) the values of columns that begin with a similar phrase into one cell with a \"|\" before using `tidy_split`. \n",
    "\n",
    "Now instead of having one row for each item with multiple linked locations, we can have one row for each linked location associated with an item.\n",
    "\n",
    "The `tidy_split` function come from [Project Cognoma](http://cognoma.org/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p>A function is a block of reusable code that is used to perform a single, related action. Learn more about functions <a href=\"https://www.w3schools.com/python/python_functions.asp\">here</a>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the values of a column and expand so that the new DataFrame has one split value per row. Filters rows where column is empty \n",
    "def tidy_split(df, column, sep='|', keep=False):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "        dataframe with the column to split and expand\n",
    "    column : str\n",
    "        the column to split and expand\n",
    "    sep : str\n",
    "        the string used to split the column's values\n",
    "    keep : bool\n",
    "        whether to retain the presplit value as it's own row\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Returns a dataframe with the same columns as `df`.\n",
    "    \"\"\"\n",
    "    indexes = list()\n",
    "    new_values = list()\n",
    "    df = df.dropna(subset=[column])\n",
    "    for i, presplit in enumerate(df[column].astype(str)):\n",
    "        values = presplit.split(sep)\n",
    "        if keep and len(values) > 1:\n",
    "            indexes.append(i)\n",
    "            new_values.append(presplit)\n",
    "        for value in values:\n",
    "            indexes.append(i)\n",
    "            new_values.append(value)\n",
    "    new_df = df.iloc[indexes, :].copy()\n",
    "    new_df[column] = new_values\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the values of columns beginnigng with a string and then use the tidy_split function to expand so that the new DataFrame has one split value per row\n",
    "def tidy_concat(df, column_starts_with, sep=\"|\"):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "        dataframe with the columns to split and expand\n",
    "    column_starts_with : str\n",
    "        the string at the beginning of the column(s) to split\n",
    "    sep : str\n",
    "        the string used to split the column's values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Returns a dataframe with the same columns as `df`.\n",
    "    \"\"\"\n",
    "    list_of_columns = df.columns.to_list()\n",
    "    columns_to_concat = [x for x in list_of_columns if x.startswith(column_starts_with)]\n",
    "    column_starts_with = column_starts_with.split('.')[1]\n",
    "    df[column_starts_with] = df[columns_to_concat[0]]\n",
    "    for column in columns_to_concat[1:]:\n",
    "        df[column_starts_with] = df[column_starts_with].astype(str) + sep + df[column].astype(str)\n",
    "    new_df = tidy_split(df, column_starts_with, sep='|')\n",
    "    new_df = new_df.drop(columns_to_concat, axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to split the values of the Type column and expand so that the new DataFrame has one split value per row\n",
    "df_places = tidy_concat(df, 'metadata.geographic_subject', sep='|')\n",
    "\n",
    "# Report the dimensionality of the dataframe (number of rows, number of columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many places are recorded in the Kaplan Collection?\n",
    "This list will include many duplicates as more than one object will be linked to a particular location. Let's drop duplicates based on the `geographic_subject` and count how many there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only items that do not have a null value in the `geographic_subject` column\n",
    "df_places = df_places[df_places['geographic_subject'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate geographic subjects from the dataframe and report the dimensionality of the dataframe.\n",
    "value = df_places.drop_duplicates(subset=['geographic_subject']).shape[0]\n",
    "\n",
    "print('There are {:,} unique locations represented in the collection.'.format(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocode `geographic_subject` Data with Nominatim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the locations on a map. First, we'll have to geocode these entries using Nominatim. \n",
    "Nominatim's API requires an email - you can input yours below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <p>In order to use this portion of the notebook, you first need to `import` modules and packages from Python for working with geographic data.</p>\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p>These modules and packages are units of code with specific tools or skills that we use in the script. If you're running this notebook on your computer, you may need to first `import` these modules within your Python interpreter. Find assistance for that <a href=\"https://packaging.python.org/tutorials/installing-packages/\">here</a>.</p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geopandas is a Python package for working with geospatial data\n",
    "import geopandas\n",
    "\n",
    "# Geopy is a Python client for several popular geocoding web services\n",
    "import geopy\n",
    "\n",
    "# From Geopy, import AsyncRateLimiter & RateLimiter to perform bulk operations gracefully\n",
    "from geopy.extra.rate_limiter import AsyncRateLimiter, RateLimiter\n",
    "\n",
    "# From Geopy, import Nominatim, a specific geocoder for OpenStreetMap data\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# NumPy is a library for adding support for manipulating and operating on large, multi-dimensional arrays\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported these modules and packages, let's prepare our dataset. Let's drop all the duplicate locations from the `geographic_subject` column and add all the linked locations to a list called `list_of_places`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all empty strings in the `Geographic Subject` column with the np.nan value. \n",
    "df_places['geographic_subject'].replace('', np.nan, inplace=True)\n",
    "\n",
    "df_places.dropna(subset=['geographic_subject'], inplace=True)\n",
    "\n",
    "# Return a list of the values in the `Geographic Subject` column and convert into a set, removing duplicate values \n",
    "list_of_places = set(df_places['geographic_subject'].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this list of locations, we will submit them to **Nominatim**, a geocoding software. We'll submit each location to Nominatim and save the latitude, longitude, and coordinate information for use in some visualizations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Set Nominatim as the geocoding web service\n",
    "locator = Nominatim(user_agent=email)\n",
    "\n",
    "list_of_places = set(df_places['geographic_subject'].to_list())\n",
    "\n",
    "#Create dictionaries for Latitude, Longitude, and Coordinates values\n",
    "lat_dict = {}\n",
    "lon_dict = {}\n",
    "coords_dict = {}\n",
    "\n",
    "# For each place in the list_of places:\n",
    "for place in list_of_places:\n",
    "    # Calculate the location\n",
    "    location = locator.geocode(place, timeout=None)\n",
    "    if location:\n",
    "    # Add a new key/value pair to the dictionary, where the key is the place and the value is latitude, longitude\n",
    "        coords_dict[place] = (location.latitude, location.longitude)\n",
    "        lat_dict[place] = location.latitude\n",
    "        lon_dict[place] = location.longitude\n",
    "    else: \n",
    "        coords_dict[place] = np.nan\n",
    "        lat_dict[place] = np.nan\n",
    "        lon_dict[place] = np.nan\n",
    "    # Give the formula 2 seconds to run again \n",
    "    time.sleep(2)\n",
    "\n",
    "# Map the dictionary into the Coordinates column by matching the Geographic Subject key\n",
    "df_places['Coordinates'] = df_places['geographic_subject'].map(coords_dict)\n",
    "df_places['Latitude'] = df_places['geographic_subject'].map(lat_dict)\n",
    "df_places['Longitude'] = df_places['geographic_subject'].map(lon_dict)\n",
    "\n",
    "# Return the first 5 lines of the df_places dataframe.\n",
    "df_places.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Geographic Subjects on a World Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have coordinates for these locations, let's make a world map. We'll be using the world map boundaries from the data provided by the `Vega datasets` Python package installed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altair only allows up to 5000 items in a map, so we're limiting this map to just one entry per Geographic Subject\n",
    "map1 = df_places.drop_duplicates(subset='geographic_subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the country boundaries data\n",
    "countries = alt.topo_feature(vega_data.world_110m.url, feature='countries')\n",
    "\n",
    "# Create the world map using the boundaries\n",
    "background = alt.Chart(countries).mark_geoshape(\n",
    "    fill='lightgray',\n",
    "    stroke='white'\n",
    ").project('equirectangular').properties(width=700)\n",
    "\n",
    "# Plot the positions of places using circles\n",
    "points = alt.Chart(map1).mark_circle(\n",
    "    \n",
    "    # Style the circles\n",
    "    size=10,\n",
    "    color='steelblue'\n",
    ").encode(\n",
    "    \n",
    "    # Provide the coordinates\n",
    "    longitude='Longitude:Q',\n",
    "    latitude='Latitude:Q',\n",
    "    \n",
    "    # Provide the location name on hover\n",
    "    tooltip=[alt.Tooltip('geographic_subject', title='Location')]\n",
    ").properties(width=700)\n",
    "\n",
    "# Layer the plotted points on top of the backgroup map\n",
    "world_chart = alt.layer(background, points)\n",
    "world_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the world chart as an html file\n",
    "save(world_chart, 'charts/world_chart.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are there items outside of America? \n",
    "\n",
    "The collection is called **Early American Judaica**, so it makes sense that the bulk of the locations are located within the current-day United States of America. How many of them are not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe that filters rows does not contain \"United States\" as part of its Geographic Subject from the df_places dataframe\n",
    "df_places = df_places[df_places['geographic_subject']!='nan']\n",
    "df_not_usa = df_places[~df_places['geographic_subject'].str.startswith(\"United States\")]\n",
    "df_not_usa = df_not_usa[df_not_usa['geographic_subject']!='nan']\n",
    "\n",
    "'{:.2%} of linked places are located outside the current-day United States of America'.format(df_not_usa.shape[0]/df_places.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich Geographic Subject Data\n",
    "\n",
    "The `geographic_subject` field now contains a string that often (but not always) includes information about the city, state, and country of item. We could split these into separate columns by hand, but an alternative is to use the geo-coordinates. Through a process known as reverse-geocoding, we can look up additional information about a city, state, and country that contains a set of coordinates and store it in a separate column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the values in the `Coordinates` column and and convert into a set\n",
    "list_of_coords = set(df_places['Coordinates'].to_list())\n",
    "\n",
    "# Create a dictionary for City, State, and Country values.\n",
    "city_dict = {}\n",
    "state_dict = {}\n",
    "country_dict = {}\n",
    "\n",
    "# Start a for-loop to iterate over each entry in list_of_coords\n",
    "for coord in list_of_coords:\n",
    "    if coord is not np.nan:\n",
    "        # Use locator to reverse-geocode\n",
    "        location = locator.reverse(coord, timeout=None)\n",
    "        time.sleep(2)\n",
    "        # Receive the raw location data and then save each specific value to its respective dictionary, with the coordinates as the key and the city/state/country as value\n",
    "        address = location.raw['address']\n",
    "        city_dict[coord] = address.get('city', '')\n",
    "        state_dict[coord] = address.get('state','')\n",
    "        country_dict[coord] = address.get('country','')\n",
    "    else: \n",
    "        # If the entry is blank or Not a Number (nan), save the value as np.nan\n",
    "        city_dict[coord] = np.nan\n",
    "        state_dict[place] = np.nan\n",
    "        country_dict[place] = np.nan\n",
    "        \n",
    "        \n",
    "# Make a new column for City, State, and Country by mapping values from the corresponding dictionary according to Coordinates column as the key        \n",
    "df_places['City'] = df_places['Coordinates'].map(city_dict)\n",
    "df_places['State'] = df_places['Coordinates'].map(state_dict)\n",
    "df_places['Country'] = df_places['Coordinates'].map(country_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work? Let's look at the `Country` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a Series containing counts of unique rows in the dataframe for each Country\n",
    "df_places['Country'].value_counts()[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Geographic Subjects on a US Map\n",
    "\n",
    "Now that we have these additional columns, we can use them to filter our data. Let's look at places where objects were created in United States of America.\n",
    "\n",
    "First we'll filter our data by `Country`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the rows that have a `Country` column value of 'United States'. \n",
    "df_places_us = df_places.loc[(df_places['Country'] == 'United States')]\n",
    "df_places_us.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a map. Note that we're changing the map layer in this chart to use just United States boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate places\n",
    "places = df_places_us[['geographic_subject', 'Latitude', 'Longitude']].drop_duplicates()\n",
    "\n",
    "# Load United States boundaries\n",
    "us = alt.topo_feature(vega_data.us_10m.url, feature='states')\n",
    "\n",
    "# Create the map of United States using the boundaries\n",
    "us_background = alt.Chart(us).mark_geoshape(\n",
    "    \n",
    "    # Style the map\n",
    "    fill='lightgray',\n",
    "    stroke='white'\n",
    ").project('equirectangular').properties(width=1500)\n",
    "\n",
    "# Plot the places\n",
    "points = alt.Chart(places).mark_circle(\n",
    "    \n",
    "    # Style circle markers\n",
    "    size=10,\n",
    "    color='steelblue'\n",
    ").encode(\n",
    "    \n",
    "    # Set position of each place using lat and lon\n",
    "    longitude='Longitude:Q',\n",
    "    latitude='Latitude:Q',\n",
    "    \n",
    "    # Provide location details on hover\n",
    "    tooltip=[alt.Tooltip('geographic_subject', title='Location')]\n",
    ").properties(width=700)\n",
    "\n",
    "# Combine map and points\n",
    "us_chart = alt.layer(us_background, points)\n",
    "us_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(us_chart, 'charts/us_chart.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Items Associated with State\n",
    "\n",
    "So far we've only looked at the places themselves, but we can also find out how many items are associated with each place. To do this, we'll group the items by `State` and count the number of grouped items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of grouped records and add as a 'State-Count' column\n",
    "df_places_us['State-Count'] = df_places_us['State'].map(df_places_us['State'].value_counts())\n",
    "df_places_us.columns.values\n",
    "\n",
    "# Drop the `City` column from the dataframe\n",
    "df_places_states = df_places_us.drop(['City'], axis=1)\n",
    "df_places_states\n",
    "\n",
    "# Drop rows with duplicate values from the `State` Column\n",
    "map2 = df_places_states.drop_duplicates(subset='State')\n",
    "map2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p>You may see a warning appear above that 'A value is trying to be set on a copy of a slice from a dataframe.' Python wants to make sure that you intended to make those changes to a copy rather than the original dataframe. In this case, you intendend to make those changes.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can map the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load United States boundaries\n",
    "usa = alt.topo_feature(vega_data.us_10m.url, feature='states')\n",
    "\n",
    "# Create the map of the United States\n",
    "us_background = alt.Chart(usa).mark_geoshape(\n",
    "    fill='lightgray',\n",
    "    stroke='white'\n",
    ").project('equirectangular').properties(width=700)\n",
    "\n",
    "# First we'll plot the created places\n",
    "points = alt.Chart(map2).mark_circle().encode(\n",
    "    \n",
    "    # Position the circles\n",
    "    longitude='Longitude:Q',\n",
    "    latitude='Latitude:Q',\n",
    "    \n",
    "    # Hover for more details about the number of items and the location\n",
    "    tooltip=[alt.Tooltip('State-Count:Q', title='Number of Items'), alt.Tooltip('State', title='State')],\n",
    "    \n",
    "    # The size of the circles is determined by the number of objects\n",
    "    size=alt.Size('State-Count:Q',\n",
    "        scale=alt.Scale(range=[0, 1000]),\n",
    "        legend=alt.Legend(title='Number of Items')\n",
    "    )\n",
    ").properties(height = 700, width=700)\n",
    "\n",
    "# Create a map by combining the background map and the points\n",
    "map_us_counts = alt.layer(us_background, points)\n",
    "map_us_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(map_us_counts, 'charts/pa_counts.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Counts by State as a CSV File\n",
    "\n",
    "It might be handy to have a CSV file that shows the count of items by `State`. Let's save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the to a comma-separated values (csv) file.\n",
    "df_places_us[['State', 'State-Count']].to_csv('data/kaplan_item_counts_by_state.csv', index=False)\n",
    "\n",
    "# Display a link to the CSV.\n",
    "display(FileLink('data/kaplan_item_counts_by_state.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Geographic Subjects on a State Map\n",
    "\n",
    "We can zoom in on Pennsylvania by using the `State` to filter the data. As before, we can then group by place and count the number of objects in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the rows that have the value of Pennsylvania in the `State` column\n",
    "df_places_pa = df_places_us.loc[(df_places_us['State'] == 'Pennsylvania')]\n",
    "\n",
    "# Count the number of grouped records by City and add as a 'City-Count' column\n",
    "df_places_pa['City-Count'] = df_places_pa['City'].map(df_places_pa['City'].value_counts())\n",
    "\n",
    "# Drop duplicates in the `City` column\n",
    "map3 = df_places_pa.drop_duplicates(subset='City')\n",
    "\n",
    "#Create a list of columns to keep\n",
    "col_list = ['geographic_subject', 'City-Count','Latitude','Longitude']\n",
    "\n",
    "# Save the dataframe to contain only the columns in col_list\n",
    "map3 = map3[col_list]\n",
    "\n",
    "# Drop any `Geographic Subject` that is only \"Pennsylvania, United States\"\n",
    "map3 = map3[map3[\"geographic_subject\"]!='United States -- Pennsylvania']\n",
    "map3.shape\n",
    "map3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p>You may see a warning appear above that 'A value is trying to be set on a copy of a slice from a dataframe.' Python wants to make sure that you intended to make those changes to a copy rather than the original dataframe. In this case, you intendend to make those changes.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are focusing on Pennsylvania, we need to load a Pennsylvania map for the site. We'll use this geojson file at the county level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_geojson = 'https://raw.githubusercontent.com/deldersveld/topojson/master/countries/us-states/PA-42-pennsylvania-counties.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can map the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PA state boundaries\n",
    "pennsylvania = alt.topo_feature(pa_geojson, 'cb_2015_pennsylvania_county_20m')\n",
    "\n",
    "# Make map of Pennsylvania\n",
    "pa_background = alt.Chart(pennsylvania).mark_geoshape(\n",
    "    fill='lightgray',\n",
    "    stroke='white'\n",
    ").project('equirectangular').properties(width=700)\n",
    "\n",
    "# Plot points for created places\n",
    "points = alt.Chart(map3).mark_circle().encode(\n",
    "    \n",
    "    # Postion the markers\n",
    "    longitude='Longitude:Q',\n",
    "    latitude='Latitude:Q',\n",
    "    \n",
    "    # Hover for more details about the number of items and the location\n",
    "    tooltip=[alt.Tooltip('geographic_subject', title='Location'),alt.Tooltip('City-Count:Q', title='Number of Items')],\n",
    "    \n",
    "    # Size determined by the number of objects\n",
    "    size=alt.Size('City-Count:Q',\n",
    "        scale=alt.Scale(range=[0, 1000]),\n",
    "        legend=alt.Legend(title='Number of Items')\n",
    "    )\n",
    ").properties(width=700)\n",
    "\n",
    "# Create a map by combining background and points\n",
    "pa_counts = alt.layer(pa_background, points)\n",
    "pa_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(pa_counts, 'charts/pa_counts.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Counts by State as a CSV File\n",
    "\n",
    "It might be handy to have a CSV file that shows the count of items by `State`. Let's save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the to a comma-separated values (csv) file.\n",
    "df_places_us[['State', 'State-Count']].to_csv('data/kaplan_item_counts_by_state.csv', index=False)\n",
    "\n",
    "# Display a link to the CSV.\n",
    "display(FileLink('kaplan_item_counts_by_state.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need Help?\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p>For additional Python and Digital Scholarship resources:</p>\n",
    "    <ul>\n",
    "        <li><a href\"https://www.w3schools.com/python/pandas/default.asp\">Pandas Tutorial from W3 Schools</a></li>\n",
    "        <li><a href\"https://altair-viz.github.io/altair-tutorial/README.html\">Altair Tutorial from W3 Schools</a></li>\n",
    "        <li><a href=\"https://guides.library.upenn.edu/digital-scholarship\">Center for Research Data and Digital Scholarship</a></li>\n",
    "    </ul>\n",
    "    <p>For help with this notebook:</p>    \n",
    "<ul>\n",
    "    <li>If you encounter any errors in this notebook, you can open an issue on GitHub or email estene@upenn.edu and reference this notebook.</li>\n",
    "\n",
    "<li>If you encounter any errors while working with the collection metadata (an incorrect date or broken ARK identifier), you can email estene@upenn.edu.</li>\n",
    "\n",
    "<li>Colenda is still a beta service. If you encounter issues with accessing any of the IIIF images or links, visit\n",
    "    <a href=\"https://colenda.library.upenn.edu/\">Colenda</a></li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Credits\n",
    "\n",
    "Created by [Emily Esten](https://www.library.upenn.edu/people/staff/emily-esten). \n",
    "\n",
    "Judaica Digital Humanities at the <a href=\"http://library.upenn.edu\">Penn Libraries</a> (also referred to as Judaica DH) is a robust program of projects and tools for experimental digital scholarship with Judaica collections, informed by digital humanities, Jewish studies, and cultural heritage approaches. Visit our [website](judaicadh.library.upenn.edu).\n",
    "\n",
    "The pre-harvested dataset for this notebook works with items from the **Arnold and Deanne Kaplan Collection of Early American Judaica**. Donated to the University of Pennsylvania Libraries in 2012 by the Kaplans, and growing each year, this collection teaches us about the everyday lives, families, communal institutions, religious organizations, voluntary associations,  businesses, and political circumstances of Jewish life throughout the western hemisphere over four centuries. More information about the collection can be found at [https://kaplan.exhibits.library.upenn.edu](https://kaplan.exhibits.library.upenn.edu). \n",
    "\n",
    "This notebook references existing code and Jupyter notebooks, including: \n",
    "* [GLAM Workbench for the National Museum of Australia](https://doi.org/10.5281/zenodo.3544747) sponsored by the [Humanities, Arts and Social Sciences (HASS) Data Enhanced Virtual Lab](https://tinker.edu.au/).\n",
    "* [Library of Congress Data Exploration: IIIF](https://github.com/LibraryOfCongress/data-exploration/blob/26510c3f4da0bc85dfa87e82141173b1830e9d64/IIIF.ipynb).\n",
    "* Gustavo Candela, María Dolores Sáez, Pilar Escobar, Manuel Marco-Such, & Rafael C.Carrasco. (2020, May 8). hibernator11/notebook-iiif-images: release1.1 (Version 1.1). Zenodo. [http://doi.org/10.5281/zenodo.3816611](https://zenodo.org/badge/latestdoi/255172461). \n",
    "* [Genes for Project Cognoma](https://github.com/cognoma/genes/blob/721204091a96e55de6dcad165d6d8265e67e2a48/2.process.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
